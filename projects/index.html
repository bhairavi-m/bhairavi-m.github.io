<!doctype html><html lang=en>
<head>
<title>Projects :: Terminal</title>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Lexical Normalization System ðŸ’¬ One of the most common methods of obtaining data used for NLP is through social media. This resource&amp;rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&amp;rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register. The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement.">
<meta name=keywords content>
<meta name=robots content="noodp">
<link rel=canonical href=/projects/>
<link rel=stylesheet href=/assets/style.css>
<link rel=stylesheet href=/assets/blue.css>
<link rel=apple-touch-icon href=/img/apple-touch-icon-192x192.png>
<link rel="shortcut icon" href=/img/favicon/blue.png>
<meta name=twitter:card content="summary">
<meta name=twitter:site content>
<meta name=twitter:creator content>
<meta property="og:locale" content="en">
<meta property="og:type" content="article">
<meta property="og:title" content="Projects">
<meta property="og:description" content="Lexical Normalization System ðŸ’¬ One of the most common methods of obtaining data used for NLP is through social media. This resource&amp;rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&amp;rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register. The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement.">
<meta property="og:url" content="/projects/">
<meta property="og:site_name" content="Terminal">
<meta property="og:image" content="/img/favicon/blue.png">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">
</head>
<body class=blue>
<div class="container center headings--one-size">
<header class=header>
<div class=header__inner>
<div class=header__logo>
<a href=/>
<div class=logo>
Bhairavi Muralidharan
</div>
</a>
</div>
<div class=menu-trigger>menu</div>
</div>
<nav class=menu>
<ul class="menu__inner menu__inner--desktop">
<li><a href=/about>About</a></li>
<li><a href=/projects>Projects</a></li>
<li><a href=/connect>Connect</a></li>
<li><a href=/resume>Resume</a></li>
</ul>
<ul class="menu__inner menu__inner--mobile">
<li><a href=/about>About</a></li>
<li><a href=/projects>Projects</a></li>
<li><a href=/connect>Connect</a></li>
<li><a href=/resume>Resume</a></li>
</ul>
</nav>
</header>
<div class=content>
<div class=post>
<h1 class=post-title>
<a href=/projects/>Projects</a></h1>
<div class=post-meta>
</div>
<div class=post-content><div>
<h1 id=lexical-normalization-system->Lexical Normalization System ðŸ’¬<a href=#lexical-normalization-system- class=hanchor arialabel=Anchor>&#8983;</a> </h1>
<p>One of the most common methods of obtaining data used for NLP is through social media. This resource&rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register. The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement. A final hybrid Character-based Encoder-Decoder model architecture was built that handled In-Vocab words and Out-Of-Vocab words differently. This method resulted in an increase in the accuracy by 3%, and I am currently working on scraping twitter data and categorizing words that fail spell check to improve the system&rsquo;s overall efficacy.</p>
<p>View the code -> <a href=https://github.com/bhairavi-m/Lexical-Normalization>GitHub</a></p>
<p>Read the report -> <a href=https://drive.google.com/file/d/1mgeCNgofnTRcDA8IjJxOsDkd0R_Dxgwa/view>Google Drive</a></p>
<h1 id=session-based-skip-prediction-spotify->Session-based Skip Prediction: Spotify ðŸŽ¹<a href=#session-based-skip-prediction-spotify- class=hanchor arialabel=Anchor>&#8983;</a> </h1>
<p>Spotify is a leading music service fuelled by its customization and music knowledge driven by algorithms to understand the way users sequentially interact with music. The main challenge for Spotify is to recommend the right music to each user. Hence, most of the work focuses on Recommender Systems and a little on describing how users sequentially interact with the streamed content they are presented. This project focussed on the task of session-based sequential skip prediction, i.e., predicting whether users will skip tracks, given their immediately preceding interactions in their listening session.</p>
<p>View the code -> <a href=https://github.com/bhairavi-m/Spotify-Sequential-Skip-Prediction>GitHub</a></p>
<p>Read the report -> <a href=https://drive.google.com/file/d/1BQT-Utcb4O52bOcUdhZ3uln9xhe2022N/view>Google Drive</a></p>
</div></div>
</div>
</div>
<footer class=footer>
<div class=footer__inner>
<div class=copyright>
<span>Â© 2022 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span>
</div>
</div>
</footer>
<script src=/assets/main.js></script>
<script src=/assets/prism.js></script>
</div>
</body>
</html>