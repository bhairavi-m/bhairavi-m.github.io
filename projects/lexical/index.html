<!doctype html><html lang=en><head><title>Lexical Normalization System ðŸ’¬ :: Bhairavi Muralidharan</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="One of the most common methods of obtaining data used for NLP is through social media. This resource&amp;rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&amp;rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register.
The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/projects/lexical/><link rel=stylesheet href=/assets/style.css><link rel=stylesheet href=/assets/blue.css><link rel=apple-touch-icon href=/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=/img/favicon/blue.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Lexical Normalization System ðŸ’¬"><meta property="og:description" content="One of the most common methods of obtaining data used for NLP is through social media. This resource&amp;rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&amp;rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register.
The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement."><meta property="og:url" content="/projects/lexical/"><meta property="og:site_name" content="Bhairavi Muralidharan"><meta property="og:image" content="/img/favicon/blue.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2021-12-20 00:00:00 +0000 UTC"></head><body class=blue><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Bhairavi Muralidharan</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/experience>Experience</a></li><li><a href=/projects>Projects</a></li><li><a href=/connect>Connect</a></li><li><a href=https://bhairavi-m.github.io/resume.pdf>Resume</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/experience>Experience</a></li><li><a href=/projects>Projects</a></li><li><a href=/connect>Connect</a></li><li><a href=https://bhairavi-m.github.io/resume.pdf>Resume</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=/projects/lexical/>Lexical Normalization System ðŸ’¬</a></h1><div class=post-meta><span class=post-date>2021-12-20
[Updated: 2021-12-20]</span></div><div class=post-content><div><p>One of the most common methods of obtaining data used for NLP is through social media. This resource&rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register.</p><p>The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement. A final hybrid Character-based Encoder-Decoder model architecture was built that handled In-Vocab words and Out-Of-Vocab words differently. This method resulted in an increase in the accuracy by 3%, and I am currently working on scraping twitter data and categorizing words that fail spell check to improve the system&rsquo;s overall efficacy.</p><p>View the code -> <a href=https://github.com/bhairavi-m/Lexical-Normalization>GitHub</a></p><p>Read the report -> <a href=https://drive.google.com/file/d/1mgeCNgofnTRcDA8IjJxOsDkd0R_Dxgwa/view>Google Drive</a></p><h4 id=team>Team<a href=#team class=hanchor arialabel=Anchor>&#8983;</a></h4><p><a href=https://www.linkedin.com/in/jonasoppenheim/>Jonas Oppenheim</a> <a href=https://www.linkedin.com/in/parthmsheth/>Parth Sheth</a></p></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>Â© 2022 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=/assets/main.js></script>
<script src=/assets/prism.js></script></div></body></html>