<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Bhairavi Muralidharan</title><link>/projects/</link><description>Recent content in Projects on Bhairavi Muralidharan</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 20 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>Lexical Normalization System ðŸ’¬</title><link>/projects/lexical/</link><pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate><guid>/projects/lexical/</guid><description>One of the most common methods of obtaining data used for NLP is through social media. This resource&amp;rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&amp;rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register.
The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement.</description><content>&lt;p>One of the most common methods of obtaining data used for NLP is through social media. This resource&amp;rsquo;s significant challenge is that the text is not traditionally accurate as it is filled with short forms and colloquial substitutes. The project&amp;rsquo;s goal was to develop a Lexical Normalization system, which enables efficient information extraction by converting non-standard text to a ready-to-use standard register.&lt;/p>
&lt;p>The process involved experimenting with data augmentation methods and implementing baselines such as Maximum Frequency Replacement. A final hybrid Character-based Encoder-Decoder model architecture was built that handled In-Vocab words and Out-Of-Vocab words differently. This method resulted in an increase in the accuracy by 3%, and I am currently working on scraping twitter data and categorizing words that fail spell check to improve the system&amp;rsquo;s overall efficacy.&lt;/p>
&lt;p>View the code -&amp;gt; &lt;a href="https://github.com/bhairavi-m/Lexical-Normalization">GitHub&lt;/a>&lt;/p>
&lt;p>Read the report -&amp;gt; &lt;a href="https://drive.google.com/file/d/1mgeCNgofnTRcDA8IjJxOsDkd0R_Dxgwa/view">Google Drive&lt;/a>&lt;/p>
&lt;h4 id="team">Team&lt;/h4>
&lt;p>&lt;a href="https://www.linkedin.com/in/jonasoppenheim/">Jonas Oppenheim&lt;/a> &lt;a href="https://www.linkedin.com/in/parthmsheth/">Parth Sheth&lt;/a>&lt;/p></content></item><item><title>Session-based Skip Prediction: Spotify ðŸŽ¹</title><link>/projects/spotify/</link><pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate><guid>/projects/spotify/</guid><description>Spotify is a leading music service fuelled by its customization and music knowledge driven by algorithms to understand the way users sequentially interact with music. This project focussed on the task of session-based sequential skip prediction, i.e., predicting whether users will skip tracks, given their immediately preceding interactions in their listening session.
This project was an exciting supervised machine learning classification problem. It helped us understand intricate behavioral patterns of how users engage with tracks and which track features play an important role in prediction.</description><content>&lt;p>Spotify is a leading music service fuelled by its customization and music knowledge driven by algorithms to understand the way users sequentially interact with music. This project focussed on the task of session-based sequential skip prediction, i.e., predicting whether users will skip tracks, given their immediately preceding interactions in their listening session.&lt;/p>
&lt;p>This project was an exciting supervised machine learning classification problem. It helped us understand intricate behavioral patterns of how users engage with tracks and which track features play an important role in prediction. It was done as part of a three-member team, and we came up with two approaches: feature-based classification using boosted trees and the second dealing with sequential modeling using RNNs. I primarily worked on setting up functions in Python (using Google Colab) that performed feature-based classification using algorithms like Random Forest Classifier, XGBoost Classifier, and Logistic Classifier. I discovered that the XGBoost classifier achieved the best validation scores across all evaluation metrics. I also preprocessed the data using PCA and conducted an error analysis for the extended models using RNNs. We individually went over all stages of the pipeline (preprocessing, feature engineering, modeling), which enabled ease of collaboration in the end and helped us iterate through different methods parallelly.&lt;/p>
&lt;p>The main challenge for Spotify is to recommend the right music to each user. Hence, most of the work focuses on Recommender Systems and a little on describing how users sequentially interact with the streamed content they are presented. The outcome of our empirical analysis highlighted the importance of sequence in the prediction and how well the recommendation system is working. Our sequential modeling using RNNs increased the accuracy from 64% to 78%, and we are currently working on tuning the hyperparameters of this Char RNN model to generate more accurate results.&lt;/p>
&lt;p>View the code -&amp;gt; &lt;a href="https://github.com/bhairavi-m/Spotify-Sequential-Skip-Prediction">GitHub&lt;/a>&lt;/p>
&lt;p>Read the report -&amp;gt; &lt;a href="https://drive.google.com/file/d/1BQT-Utcb4O52bOcUdhZ3uln9xhe2022N/view">Google Drive&lt;/a>&lt;/p>
&lt;h4 id="team">Team&lt;/h4>
&lt;p>&lt;a href="https://www.linkedin.com/in/purva-sheth/">Purva Sheth&lt;/a> &lt;a href="https://www.linkedin.com/in/parthmsheth/">Parth Sheth&lt;/a>&lt;/p></content></item></channel></rss>